{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - Leonardo\n",
    "\n",
    "* Estudos em RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "\n",
    "import spacy\n",
    "import re, string, unicodedata\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.utils as ku\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class setup(object):\n",
    "    def __init__(self):\n",
    "        self.path = 'introducao-CNN-master/intro_NLP-master/exercicios/data/text_generation/sonnets.txt'\n",
    "        self.data_1 = open(self.path).read()\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.corpus = self.data_1.lower().split(\"\\n\")\n",
    "        return self.data_1\n",
    "    \n",
    "    def tokenize_plis(self, reverse:bool =False):\n",
    "           \n",
    "        self.tokenizer = Tokenizer(num_words = 250, lower=True)\n",
    "        self.tokenizer.fit_on_texts(self.corpus)\n",
    "        self.seqs = self.tokenizer.texts_to_sequences(self.corpus)                 \n",
    "        return self.seqs\n",
    "    \n",
    "    def destokenize_plis(self,seqs):\n",
    "        res_seqs = []\n",
    "        for sentence in seqs:\n",
    "            reverse = dict(map(reversed, self.tokenizer.word_index.items()))\n",
    "            words = [reverse.get(word) for word in sentence]\n",
    "            res_seqs.append(words)\n",
    "        return res_seqs\n",
    "\n",
    "\n",
    "\n",
    "a = setup()\n",
    "data = a.get_data()\n",
    "data_tokenized = a.tokenize_plis()\n",
    "b = a.destokenize_plis(data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "68/68 [==============================] - 8s 116ms/step - loss: 0.1291\n",
      "Epoch 2/20\n",
      "68/68 [==============================] - 8s 121ms/step - loss: 3.7871e-04\n",
      "Epoch 3/20\n",
      "68/68 [==============================] - 8s 123ms/step - loss: 1.9372e-04\n",
      "Epoch 4/20\n",
      "68/68 [==============================] - 8s 120ms/step - loss: 1.9214e-04\n",
      "Epoch 5/20\n",
      "68/68 [==============================] - 8s 119ms/step - loss: 1.9208e-04\n",
      "Epoch 6/20\n",
      "68/68 [==============================] - 8s 121ms/step - loss: 1.9208e-04\n",
      "Epoch 7/20\n",
      "68/68 [==============================] - 8s 121ms/step - loss: 1.9208e-04\n",
      "Epoch 8/20\n",
      "68/68 [==============================] - 8s 122ms/step - loss: 1.9208e-04\n",
      "Epoch 9/20\n",
      "68/68 [==============================] - 8s 120ms/step - loss: 1.9208e-04\n",
      "Epoch 10/20\n",
      "68/68 [==============================] - 8s 120ms/step - loss: 1.9208e-04\n",
      "Epoch 11/20\n",
      "68/68 [==============================] - 8s 119ms/step - loss: 1.9208e-04\n",
      "Epoch 12/20\n",
      "68/68 [==============================] - 8s 117ms/step - loss: 1.9208e-04\n",
      "Epoch 13/20\n",
      "68/68 [==============================] - 8s 119ms/step - loss: 1.9208e-04\n",
      "Epoch 14/20\n",
      "68/68 [==============================] - 8s 121ms/step - loss: 1.9208e-04\n",
      "Epoch 15/20\n",
      "68/68 [==============================] - 8s 118ms/step - loss: 1.9208e-04\n",
      "Epoch 16/20\n",
      "68/68 [==============================] - 8s 118ms/step - loss: 1.9208e-04\n",
      "Epoch 17/20\n",
      "68/68 [==============================] - 8s 118ms/step - loss: 1.9208e-04\n",
      "Epoch 18/20\n",
      "68/68 [==============================] - 8s 119ms/step - loss: 1.9208e-04\n",
      "Epoch 19/20\n",
      "68/68 [==============================] - 8s 119ms/step - loss: 1.9208e-04\n",
      "Epoch 20/20\n",
      "68/68 [==============================] - 8s 119ms/step - loss: 1.9208e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8650e66a60>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 250\n",
    "padded_dataset = pad_sequences(data_tokenized, maxlen=200, padding='post', truncating='post')\n",
    "train_x = np.array(padded_dataset[:,:])\n",
    "train_y = padded_dataset[:,-1]\n",
    "train_y = ku.to_categorical(train_y, num_classes = 250)\n",
    "########################################################################\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=250, output_dim=128, embeddings_regularizer=regularizers.l2(0.001)))\n",
    "model.add(LSTM(128, input_dim=128, return_sequences=False))\n",
    "model.add(Dense(250, input_dim=128, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.RMSprop(lr=0.005))\n",
    "model.fit(train_x, train_y, epochs=20, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
